---
title: "R Notebook Text Mining"
output: html_notebook
---


```{r}
################################################################################
######################### Projet Text Mining ###################################
################################################################################

############################# Introduction #####################################

# Ce notebook a été rédigé par Edwin Charotte et Arnaud Mouttapa dans le cadre 
# du cours de Text Mining enseigné à CY TECH par M. Senoussi.
# Le notebook comprend les trois première parties du sujet, qui devaient être 
# rédigées en R.

# Dans ce notebook, nous étudions le dataset abcnews. Le dataset est un corpus 
# de documents qui sont des têtes d'affiches d'articles de la Australian 
# Broadcasting Corporation, qui est le service public australien de nouvelles.
# Les données couvrent les têtes d'affiche du 19/02/2003 au 31/12/2021 et 
# traitent différents thèmes que nous allons étudier.


# Imports

library(tm)
library(wordcloud)
library(coop) # Pour la fonction sparsity

# Lecture du jeu de données

data = read.csv("abcnews-date-text.csv", header = TRUE, sep = ",")
head(data)
```


```{r}
##################### Partie I : De quoi parlait-on en ... ? ###################

# Données entre 01/02/2003 et 31/12/2019
# filter = (data['publish_date'] > 20030201) & (data['publish_date'] <= 20191231)

# Données entre 01/03/2008 et 31/08/2008
filter2 = (data['publish_date'] >20080301) & (data['publish_date'] <= 20080831)

# data_filtered = data[filter,]

data_filtered_2008_2 = data[filter2,]

# Conversion en VCorpus
Vdata = VCorpus(VectorSource(data_filtered_2008_2$headline_text))

# Nettoyage du contenu : suppression des stopwords, de la ponctuation et 
# conversion en caractères minuscules

custom_stopwords <- c("is","a","for","in")

cleaned_corpus = tm_map(Vdata, content_transformer(tolower))
cleaned_corpus = tm_map(cleaned_corpus,removePunctuation)
cleaned_corpus = tm_map(cleaned_corpus, removeNumbers)
cleaned_corpus = tm_map(cleaned_corpus, removeWords, stopwords('english'))
cleaned_corpus = tm_map(cleaned_corpus, removeWords, custom_stopwords)

# Pour vérifier que le nettoyage a été appliqué
# content(cleaned_corpus[[1]])
# content(cleaned_corpus[[2]])

# Transformation en bag of words
dtm <- DocumentTermMatrix(cleaned_corpus)
matrix <- as.matrix(dtm)

print(paste("Dimension du bag of words : ",list(dim(matrix))))

# sparsity(matrix) # 0.9997488

frequency <- colSums(matrix)
frequency <-sort(frequency, decreasing = TRUE)
print(paste("Mots les plus fréquents : ",list(head(frequency))))
```


```{r}
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100], color = brewer.pal(8,"Dark2"))
```
```{r}
## Commentaires

# On remarque que police n'est pas présent dans le wordcloud
# Il s'agit du mot le plus fréquent. Le wordcloud n'est pas assez grand
# pour caser le mot police dans le wordcloud.
# On retrouve en général des mots liés à l'Australie (qld, nrl, Sydney)
# On retrouve également le mot "Olympics", ce qui était attendu pour l'année 2008
# On retrouve beaucoup de mots du registre de la politique et de l'économie : 
# "govt", "interview", "death", "talks", "man", "court"...
```




```{r}
##### Partie II : Partie II : Associer des classes à des dépêches ##############

#### Création d'un dataframe avec les sujets Iraq, Covid, Climate et Other #####

library(dplyr)
library(stringr)

# Df
df <- data %>%
  mutate(topic = case_when(
    str_detect(headline_text, "\\biraq\\b") ~ "Iraq", 
    str_detect(headline_text, "\\bcovid\\b") ~ "Covid",
    str_detect(headline_text, "\\bolympic\\b") ~ "Olympic",
    TRUE ~ "Other"
  ))
# On supprime les lignes qui contiennent le sujet "Other"
df <- df %>%
  filter(topic != "Other")
```


```{r}
################### On affiche les sujets et leur wordcloud ####################
library(tm)
library(wordcloud)

# On divise le dataframe par sujet
data_split <- split(df$headline_text, df$topic)

# Fonction pour nettoyer et calculer les fréquences de mots
get_word_frequencies <- function(texts) {
  corpus <- Corpus(VectorSource(texts))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  dtm <- DocumentTermMatrix(corpus)
  frequency <- colSums(as.matrix(dtm))
  frequency <- sort(frequency, decreasing = TRUE)
  return(frequency)
}

# Calcul des fréquences de mots pour chaque sujet
frequencies <- lapply(data_split, get_word_frequencies)

# Wordcloud pour chaque sujet
for (topic in names(frequencies)) {
  frequency <- frequencies[[topic]]
  words <- names(frequency)
  wordcloud(words[1:100], frequency[1:100], main = paste("Wordcloud for topic:",
                                                         topic))
}
```
```{r}
# On a représenté ci-dessus des wordclouds pour chaque catégorie de documents : 
# par exemple, pour tous les documents contenant le mot "Covid", on a recensé 
# les mots les plus fréquents, représentés sous la forme d'un nuage de mots.
```


```{r}
###################### Modèle Bayesien ########################################

### On regarde la proportion de chaque classe
class_counts <- table(df$topic)
class_proportions <- class_counts / nrow(df)
print(paste("Proportion de chaque classe : ", (list(class_proportions))))
```


```{r}
###### Train/Test
# Charger les packages

library(tibble)
library(caret)

set.seed(25) 
trainIndex <- createDataPartition(df$topic, p = .8, 
                                  list = FALSE, 
                                  times = 1, 
                        
                                  )

trainSet <- df[trainIndex, ]
testSet  <- df[-trainIndex, ]
```


```{r}
## Verification des proportions dans le train set et le test set

# Train set
train_class_counts <- table(trainSet$topic)
train_class_proportions <- train_class_counts / nrow(trainSet)
print("Proportion des classes dans le train set : ")
print(train_class_proportions)
```


```{r}
# Test set
test_class_counts <- table(testSet$topic)
test_class_proportions <- test_class_counts / nrow(testSet)
print("Proportion des classes dans le test set")
print(test_class_proportions)

# Les proportions sont similaires dans le train et le test
```


```{r}
### Model bayesien
# Pour eviter le sur-apprentissage et le biais, on supprime les mots Iraq, Covid et Olympic

library(stringr)
trainSet <- trainSet %>%
  mutate(headline_text = str_replace_all(headline_text, "\\b(iraq|covid|olympic)\\b", ""))

# Corpus
crp = Corpus(VectorSource(trainSet$headline_text))

# Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)

# Matrice
matrice = as.matrix(DocumentTermMatrix(clean_crp))

# Création du modèle

library(naivebayes)

model <- multinomial_naive_bayes(y=trainSet$topic,x=matrice)

# Préparation des données de test de la même manière
test_corpus <- Corpus(VectorSource(testSet$headline_text))
clean_crp <- tm_map(test_corpus, content_transformer(tolower))
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)

# Matrice
test_dtm <- DocumentTermMatrix(clean_crp,
                               control = list(dictionary =
                                          Terms(DocumentTermMatrix(clean_crp))))

# Prédiction
predictions <- predict(model, newdata = as.matrix(test_dtm))
# Afficher les prédictions
print("Prédictions : ")
print(predictions)
```


```{r}
############## Accuracy, precision, recall, F1 ##############################

# Matrice confusion
print("Matrice de confusion : ")
cat("\n")
(Confusion=table(testSet$topic,predictions))
cat("\n")

# Accuracy  
print("Accuracy : ")
sum(diag(Confusion))/sum(Confusion)
cat("\n")

# précision, rappel et score F1 pour chaque classe

precision <- diag(Confusion) / colSums(Confusion)
recall <- diag(Confusion) / rowSums(Confusion)
f1 <- 2 * precision * recall / (precision + recall)

# Afficher les résultats

print("Précision : ")
print(precision)
cat("\n")
print("Recall : ")
print(recall)
cat("\n")
print("F1 Score : ")
print(f1)

```
```{r}
# Le jeu de données qu'on utilise pour cette partie est le corpus dont les 
# documents contiennent au moins un des trois mots suivants : "iraq", "covid" ou
# "olympics".
# On effectue un train-test split sur ces documents, et on entraîne un 
# classifieur bayésien multinomial sur les données d'entraînement auquel on a
# retiré les mots "iraq", "covid" et "olympics" pour éviter un biais dû à 
# la présence systématique de ces mots dans le jeu de test.
```


```{r}
############### Partie III : Définir des clusters de dépêches ##################

### Définition des fonctions pour l'algorithme Frequent Term Sets

# Fonction Cov
coverage <- function(dtm, termSets){
  # dtm est un DocumentTermMatrix
  # termSets est une concaténation de mots. Par exemple : c("banane","pomme")
  # Renvoie la liste des documents de dtm contenant les mots de termSet
  
  if(length(termSets) == 0){return(c())}
  else{
    listDocs = c()
    i = 588
    for(i in as.integer(dtm$dimnames$Docs)){
      positionList = which(dtm$i == i)
      termsOfDocIndexList = dtm$j[positionList]
      docTerms = dtm$dimnames$Terms[termsOfDocIndexList]
    
      if (all(termSets %in% docTerms)){
        listDocs <-c(listDocs, i)
      }
    }
  }
  return(listDocs)
}

# Standard overlap
standardOverlap <- function(frequentTermSet, cluster, dtm){
  # frequentTermSet est une chaîne de caractères
  # cluster est une concaténation d'entiers : c(1,5,8), les chiffres
  # désignent le numéro du document dans dtm, appartenant au cluster
  # dtm est un DocumentTermMatrix
  if(is.null(cluster)){return(Inf)}
  value = 0
  for(doc in cluster){
    # Pour obtenir la liste des mots du doc
    positionList = which(dtm$i == doc)
    termsOfDocIndexList = dtm$j[positionList]
    docTerms = dtm$dimnames$Terms[termsOfDocIndexList]
    
    # Compter le nombre de frequent terms dans chaque doc
    frequentTermInDocBoolList = frequentTermSet %in% docTerms 
    numFreqTermsInDoc = length(frequentTermInDocBoolList[
      frequentTermInDocBoolList == TRUE
    ])
    value <- value + numFreqTermsInDoc - 1
  }
  result = value/length(cluster)
  return(result)
}

# Fonction de suppression de documents dans le DTM
removeFromDTM <- function(bestCandidate, dtm){
  cov <- coverage(dtm, bestCandidate)
  
  # Si la couverture est nulle, il n'y a rien à faire
  if(is.null(cov)){return(dtm)}
  for(i in cov){
    listIndexToRemove = which(dtm$i == i)
    dtm$dimnames$Docs = dtm$dimnames$Docs[
      -which(dtm$dimnames$Docs == as.character(i))
      ]
    
    dtm$i = dtm$i[-listIndexToRemove]
    dtm$j = dtm$j[-listIndexToRemove]
    dtm$v = dtm$v[-listIndexToRemove]
  }
  
  dtm$nrow = dtm$nrow - length(cov)
  
  return(dtm)
}

# Fonction de suppression des documents qui ne contiennent aucun frequent term
earlyDTMClean = function(frequentTermSet, dtm){
  cov <- c()
  for(term in frequentTermSet){
    cov <- c(cov, coverage(dtm, term))
  }
  cov <- unique(cov)
  list_docs_to_remove = c()
  for(i in dtm$dimnames$Docs){
    if(!(i %in% cov)){
      list_docs_to_remove <-c(list_docs_to_remove, i)
    }
  }
  dtm$dimnames$Docs = dtm$dimnames$Docs[-as.integer(list_docs_to_remove)]
  
  for(i in list_docs_to_remove){
    positionList = which(dtm$i == i)
    
    dtm$i = dtm$i[-positionList]
    dtm$j = dtm$j[-positionList]
    dtm$v = dtm$v[-positionList]
    
  }
  dtm$nrow = dtm$nrow - length(list_docs_to_remove)
  
  return(dtm)
}

# Algorithme principal Frequent Term Sets
selectTermSets <- function(dtm, minsup = 15){
  selectedTermSets = c()
  listCoverages = c()
  n = length(dtm$dimnames$Docs)
  
  # Renvoie les termes apparaissant au moins minsup fois dans le dtm
  remainingTermSet = findFreqTerms(dtm,lowfreq = minsup)
  cov = c()
  for(term in remainingTermSet){
    cov <- c(cov, coverage(dtm, term))
  }
  cov <- unique(cov)
  dtm = earlyDTMClean(remainingTermSet, dtm)
  # Tant qu'il y a des candidats à ajouter et que la couverture des 
  # frequent term n'est pas entière
  while(length(remainingTermSet) > 0 & length(coverage(dtm, selectedTermSets))!=n){
    listOverlaps = c()
    termSet = remainingTermSet[[1]]
    for(termSet in remainingTermSet){
      cluster = coverage(dtm, termSet)
      listOverlaps <- c(listOverlaps, standardOverlap(remainingTermSet,
                                                      cluster,
                                                      dtm))
    }
    # Selection du candidat contribuant à un overlap minimal
    bestCandidateIndex = which.min(listOverlaps)
    bestWords = remainingTermSet[[bestCandidateIndex]]
    selectedTermSets <- c(selectedTermSets, 
                          list(bestWords))
    listCoverages = c(listCoverages, 
                      list(coverage(dtm, bestWords)))
    
    # Suppression de la couverture du meilleur mot du dtm
    dtm = removeFromDTM(bestWords, dtm)
    list_sets_to_remove = c()
    for(i in 1:length(remainingTermSet)){
      if(all(bestWords %in% remainingTermSet[[i]])){
        list_sets_to_remove = c(list_sets_to_remove,i)
      }
    }
    remainingTermSet = remainingTermSet[-list_sets_to_remove]
  }
  
  print(paste("Couverture entière des documents : ", length(dtm$dimnames$Docs)==0))
  print(paste("Documents n'appartenant à aucun cluster : ", 
              intersect(as.character(cov), dtm$dimnames$Docs)))
  
  return(mapply(c,selectedTermSets, listCoverages))
}
```


```{r}
# Données entre 01/03/2008 et 04/03/2008
filter3 = (data['publish_date'] >20080301) & (data['publish_date'] <= 20080304)

data_filtered_2008_3 = data[filter3,]

# Conversion en VCorpus
Vdata = VCorpus(VectorSource(data_filtered_2008_3$headline_text))

# Nettoyage du contenu : suppression des stopwords, de la ponctuation et 
# conversion en caractères minuscules

custom_stopwords <- c("is","a","for","in")

cleaned_corpus = tm_map(Vdata, content_transformer(tolower))
cleaned_corpus = tm_map(cleaned_corpus,removePunctuation)
cleaned_corpus = tm_map(cleaned_corpus, removeNumbers)
cleaned_corpus = tm_map(cleaned_corpus, removeWords, stopwords('english'))
cleaned_corpus = tm_map(cleaned_corpus, removeWords, custom_stopwords)

# Transformation en Document Term Matrix
dtmbis <- DocumentTermMatrix(cleaned_corpus)

```


```{r}
findFreqTerms(dtmbis,15)
selectTermSets(dtmbis,15)
```
```{r}
# Dans cette partie, nous avons appliqué l'algorithme de Frequent Term Set 
# comme vu en cours. Il s'agit de renvoyer les ensembles de termes les plus 
# fréquents dans le corpus ainsi que leur couverture associée.

# Le jeu de données utilisé est un sous ensemble du Document Term Matrix initial
# auquel on a retiré les documents qui ne contenaient pas au moins un des 
# frequent terms. Cette opération est effectuée pour se ramener à la situation 
# du cours. De plus, le jeu de données à été restreint à 4 jours pour réduire
# le temps d'exécution de l'algorithme.

# Dans notre cas, nous avons observé que les ensembles sélectionnés n'étaient 
# que des singletons, nous avons donc retiré la considération des combinaisons
# de frequent terms lors de l'évaluation du standard overlap et du meilleur 
# candidat. 

# On observe que tous les mots appartenant au mots candidats sont sélectionnés
# pour constituer les Selected Term Sets. On retrouve également pour chaque mot
# sa couverture associée.

```

