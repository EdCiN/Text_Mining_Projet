library("FactoMineR")
###########
dim(iris)
names(iris)
str(iris)
attributes(iris)
set.seed(1234)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
trainData
testData
install.packages("h:/Downloads/randomForest_4.7-1.1.tar.gz", repos = NULL, type = "source")
install.packages("randomForest)
library(randomForest)
,
;
.
rf<-randomForest(Species~data=trainData,ntree=100)
table(predict(rf), trainData$Species)
print(rf)
install.packages("randomForest)
install.packages("randomForest")
setwd("C:/Users/CYTech Student/ING3/text_minig/Projet")
############################### Import #########################################
library(NLP)
library("tm")
library(RColorBrewer)
library(wordcloud)
library(readr)
data <- read_csv("abcnews-date-text.csv")
############################### Pre process ###################################
data2=data # data test
#dtype
sapply(data, class)
#Format date
data$publish_date <- as.character(data2$publish_date)
data$publish_date <- as.Date(data2$publish_date, format="%Y%m%d")
data2=data # data test
# Définir les dates de début et de fin
date_debut <- as.Date("2003-02-01")
date_fin <- as.Date("2003-07-07")
View(data)
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= start_date & data$publish_date <= end_date, ]
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= date_debut & data$publish_date <= date_fin, ]
#corpus
crp=VCorpus(VectorSource(data_fev_juillet$headline_text))
#Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
stopwords(kind="en")
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#
content(crp[1])
#
content(crp[[1]])
data_fev_juillet
View(data_fev_juillet)
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= date_debut & data$publish_date <= date_fin, ]
data <- read_csv("abcnews-date-text.csv")
############################### Pre process ###################################
#dtype
sapply(data, class)
#Format date
data$publish_date <- as.character(data$publish_date)
data$publish_date <- as.Date(data$publish_date, format="%Y%m%d")
View(data)
############################## Partie I ##############################
# Définir les dates de début et de fin
date_debut <- as.Date("2003-02-01")
date_fin <- as.Date("2003-08-01")
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= date_debut & data$publish_date <= date_fin, ]
#corpus
crp=VCorpus(VectorSource(data_fev_juillet$headline_text))
#Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
stopwords(kind="en")
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#
content(crp[[1]])
content(clean_crp[[1]])
#matrice
matrice=DocumentTermMatrix(clean_crp)
#
matrice
matrice=as.matrix(matrice)
############################## Partie I ##############################
# Définir les dates de début et de fin
date_debut <- as.Date("2003-02-01")
date_fin <- as.Date("2003-05-01")
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= date_debut & data$publish_date <= date_fin, ]
#corpus
crp=VCorpus(VectorSource(data_fev_juillet$headline_text))
#Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
stopwords(kind="en")
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#
content(crp[[1]])
content(clean_crp[[1]])
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice)
matrice
# Calculer la fréquence des mots
word_freq <- col_sums(as.matrix(matrice))
# Calculer la fréquence des mots
word_freq <- col_sums(matrice)
# Calculer la fréquence des mots
word_freq <- col_Sums(matrice)
# Calculer la fréquence des mots
word_freq <- colSums(matrice)
# Trier les mots par fréquence
word_freq <- sort(word_freq, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
wordcloud(words[1:100], frequency[1:100])
wordcloud(words[1:100], word_freq[1:100])
# Trier les mots par fréquence
frequency <- sort(word_freq, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], word_freq[1:100])
library(NLP)
library("tm")
library(RColorBrewer)
library(wordcloud)
library(readr)
data <- read_csv("abcnews-date-text.csv")
############################### Pre process ###################################
#dtype
sapply(data, class)
#Format date
data$publish_date <- as.character(data$publish_date)
data$publish_date <- as.Date(data$publish_date, format="%Y%m%d")
View(data)
install.packages("textdata")
# Charger le package
library(textdata)
library(textdata)
#stopwords supplémentaires
stopwords_supp <- c("new", "title",textdata::stopwords(kind = "en-verbs"))
#stopwords supplémentaires
verb=textdata::stopwords(kind = "en-verbs")
clean_crp <- tm_map(clean_crp, removeNumbers)
install.packages("english")
library(english)
#stopwords supplémentaires
verb=english::english_verbs$base
stopwords_supp <- c("new", "title","will","concerns","talks","help","nsw","act","set","report","get","dies","found","first")
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp)
############################## Partie I ##############################
# Définir les dates de début et de fin
date_debut <- as.Date("2003-02-01")
date_fin <- as.Date("2003-05-01")
#data filtrer par la période
data_fev_juillet=data[data$publish_date >= date_debut & data$publish_date <= date_fin, ]
#corpus
crp=VCorpus(VectorSource(data_fev_juillet$headline_text))
#Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
#stopwords supplémentaires
stopwords_supp <- c("new", "title","will","concerns","talks","help","nsw","act","set","report","get","dies","found","first")
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp)
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice) # lourd genre 1gigas
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], word_freq[1:100])
wordcloud(words[1:100], frequency[1:100])
#stopwords supplémentaires
stopwords_supp <- c("new", "title","will","concerns","talks","help","nsw","act",
"set","report","get","dies","found","first","says","new")
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp)
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice) # lourd genre 1gigas
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp,new)
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice) # lourd genre 1gigas
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
#stopwords supplémentaires
stopwords_supp <- c(new, title,will,concerns,talks,help,nsw,ac,
set,report,get,dies,found,first,says,will,take,continues,found)
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp,new, )
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp)
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice) # lourd genre 1gigas
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
warnings()
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
# Ajouter les stopwords supplémentaires à la liste des stopwords existants
all_stopwords <- c(stopwords("english"), stopwords_supp)
clean_crp <- tm_map(clean_crp, removeWords, all_stopwords)
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=DocumentTermMatrix(clean_crp)
matrice=as.matrix(matrice) # lourd genre 1gigas
# Calculer la fréquence des mots
frequency <- colSums(matrice)
# Trier les mots par fréquence
frequency <- sort(frequency, decreasing = TRUE)
#Nuage de mots
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
library(caret)
# Filtrer les dépêches pour les sujets d'intérêt
df_filtered <- data[grep("Irak|covid|sport", data$text_column, ignore.case = TRUE), ]
# Filtrer les dépêches pour les sujets d'intérêt
df_filtered <- data[grep("Irak|covid|sport", data$headline_text, ignore.case = TRUE), ]
View(df_filtered)
############################### Import #########################################
library(NLP)
library("tm")
library(RColorBrewer)
library(wordcloud)
library(readr)
data <- read_csv("abcnews-date-text.csv")
############################### Pre process ###################################
#dtype
sapply(data, class)
#Format date
data$publish_date <- as.character(data$publish_date)
data$publish_date <- as.Date(data$publish_date, format="%Y%m%d")
library(dplyr)
library(stringr)
# Ajouter une nouvelle colonne qui indique le sujet de chaque dépêche
df <- data %>%
mutate(topic = case_when(
str_detect(headline_text, "\\biraq\\b") ~ "Iraq",
str_detect(headline_text, "\\bcovid\\b") ~ "Covid",
str_detect(headline_text, "\\bolympics\\b") ~ "JO",
TRUE ~ "Other"
))
# Supprimer les lignes qui contiennent le sujet "Other"
df <- df %>%
filter(topic != "Other")
library(tm)
library(wordcloud)
# Divisez votre dataframe par sujet
data_split <- split(df$headline_text, df$topic)
# Créez une fonction pour nettoyer et calculer les fréquences de mots
get_word_frequencies <- function(texts) {
corpus <- Corpus(VectorSource(texts))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("french"))
dtm <- DocumentTermMatrix(corpus)
frequency <- colSums(as.matrix(dtm))
frequency <- sort(frequency, decreasing = TRUE)
return(frequency)
}
# Calculez les fréquences de mots pour chaque sujet
frequencies <- lapply(data_split, get_word_frequencies)
# Créez une wordcloud pour chaque sujet
for (topic in names(frequencies)) {
frequency <- frequencies[[topic]]
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100], main = paste("Wordcloud for topic:", topic))
}
df <- df[ , -which(names(df) %in% "publish_date")]
###### Train/Test
# Charger le package
library(tibble)
library(caret)
trainIndex <- createDataPartition(df$topic, p = .8,
list = FALSE,
times = 1,
)
trainSet <- df[trainIndex, ]
testSet  <- df[-trainIndex, ]
library(e1071)
#Corpus
crp= Corpus(VectorSource(trainSet$headline_text))
#Cleaning
clean_crp <- tm_map(crp, content_transformer(tolower))
stopwords(kind="en")
clean_crp <- tm_map(clean_crp, removeWords, stopwords("english"))
clean_crp <- tm_map(clean_crp, removePunctuation)
clean_crp <- tm_map(clean_crp, removeNumbers)
#matrice
matrice=as.matrix(DocumentTermMatrix(clean_crp))
train_dtm_df=as.data.frame(matrice)
# Ajouter la colonne de classe au dataframe
train_dtm_df$topic <- trainSet$topic
model <- naiveBayes(topic ~ ., data = train_dtm_df)
model
predict(model, newdata =train_dtm_df)
